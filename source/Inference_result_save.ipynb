{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# TODO: remove and import from diffusers.utils when the new version of diffusers is released\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import json\n",
    "import SimpleITK as sitk\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            json_file,\n",
    "            data_root, \n",
    "            size=512,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.size = size\n",
    "\n",
    "        self.data = []\n",
    "        with open(json_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = self.data[i]\n",
    "        image_name = item[\"preImg\"]\n",
    "        text = item['prompt']\n",
    "\n",
    "        # baseline_name = image_name.split(\"_\")[0]+\"_M00\"\n",
    "        # if os.path.isfile(os.path.join(self.data_root, baseline_name+\".nii.gz\")):\n",
    "        #     image_name = baseline_name\n",
    "        \n",
    "        image = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(self.data_root, image_name+\".nii.gz\")))\n",
    "        image = np.repeat(image[..., np.newaxis], 3, axis=-1)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        target_image_name = item[\"img\"]\n",
    "        target_image = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(self.data_root, target_image_name+\".nii.gz\")))\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text\": text,\n",
    "            \"target_image\": target_image,\n",
    "            \"image_name\": image_name,\n",
    "            \"target_image_name\": target_image_name,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_tokens = [\n",
    "    \"<DX-CN>\",\n",
    "    \"<DX-MCI>\",\n",
    "    \"<DX-AD>\",\n",
    "    \"<Age--64>\",\n",
    "    \"<Age-65-74>\",\n",
    "    \"<Age-75-84>\",\n",
    "    \"<Age-85->\",\n",
    "    \"<Gender-Male>\",\n",
    "    \"<Gender-Female>\",\n",
    "    \"<Edu-6-12>\",\n",
    "    \"<Edu-13-16>\",\n",
    "    \"<Edu-17-18>\",\n",
    "    \"<Edu-19->\",\n",
    "    \"<Race-White>\",\n",
    "    \"<Race-Black>\",\n",
    "    \"<Race-Asian>\",\n",
    "    \"<Race-MoreThanOne>\",\n",
    "    \"<Race-Unknown>\",\n",
    "    \"<Race-Indian-Alaskan>\",\n",
    "    \"<Race-Hawwaiian-otherPI>\",\n",
    "    \"<Marry-Married>\",\n",
    "    \"<Marry-Widowed>\",\n",
    "    \"<Marry-Divorced>\",\n",
    "    \"<Marry-NeverMarried>\",\n",
    "    \"<Marry-Unknown>\",\n",
    "    \"<APOE4-0>\",\n",
    "    \"<APOE4-1>\",\n",
    "    \"<APOE4-2>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "checkpoint_path = \"/inye/results/TextInversion_ADNI3_3/checkpoint-last\"\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "device = torch.device(\"cuda\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(checkpoint_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    checkpoint_path, subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added_tokens = tokenizer.add_tokens(placeholder_tokens)\n",
    "print(num_added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_json_file = \"/inye/dataset/ADNI3_test_metadata2.jsonl\"\n",
    "data_dir = \"/inye/dataset/T1_2D_slice_512/\"\n",
    "resolution = 512\n",
    "\n",
    "val_dataset = ValDataset(\n",
    "    json_file=test_data_json_file,\n",
    "    data_root=data_dir,\n",
    "    size=resolution,\n",
    ")\n",
    "\n",
    "pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    "    safety_checker=None,\n",
    "    torch_dtype=weight_dtype,\n",
    ")\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline = pipeline.to(device)\n",
    "pipeline.set_progress_bar_config(disable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "result_folder = '/inye/Inference/seed-42_strength-0.75_guidance-3.0_step-100'\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(len(val_dataset))):\n",
    "    val_example = val_dataset[i]\n",
    "    img_name = val_example[\"image_name\"]\n",
    "    target_img_name = val_example[\"target_image_name\"]\n",
    "    input_image = val_example[\"image\"].unsqueeze(0)\n",
    "    target_image = val_example[\"target_image\"]\n",
    "    text = val_example[\"text\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_latents = vae.encode(input_image.to(device, dtype=weight_dtype)).latent_dist.sample()\n",
    "        input_latents = input_latents * vae.config.scaling_factor\n",
    "\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        result_image = pipeline(\n",
    "            prompt=text,\n",
    "            image = input_latents,\n",
    "            num_steps=100,\n",
    "            strength=0.75,\n",
    "            guidance_scale=3.0,\n",
    "            output_type=\"np\",\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "\n",
    "    result_image = result_image[:,:,0]\n",
    "    result_image = sitk.GetImageFromArray(result_image)\n",
    "    sitk.WriteImage(result_image, os.path.join(result_folder, target_img_name+\".nii.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(input_image[0,0], cmap=\"gray\", vmin=-1, vmax=1)\n",
    "ax[0].set_title(\"Input Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(target_image, cmap=\"gray\")\n",
    "ax[1].set_title(\"Target Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "result_image = sitk.GetArrayFromImage(result_image)\n",
    "ax[2].imshow(result_image, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[2].set_title(\"Output Image\")\n",
    "ax[2].axis(\"off\")\n",
    "\n",
    "print(\"Image Name:\", img_name)\n",
    "print(\"Target Image Name:\", target_img_name)\n",
    "print(\"Text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = '/inye/Inference/strength_0.3_guidance_2.0'\n",
    "os.makedirs(result_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image[:,:,0].min(), result_image[:,:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
